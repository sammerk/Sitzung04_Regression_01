[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AQUA-d Forum",
    "section": "",
    "text": "1 Willkommen üëãüèº\nHier finden sich\n\nVideos,\nAufgaben,\nDaten und\nCode\n\nf√ºr die asynchrone Vorbereitung der Sitzung Regression 01 im AQUA-d Forum des SoSe 24. Ich bitte euch alle vorab das Kapitel Einfache und multiple lineare Regression durchzuarbeiten, also die Videos zu rezipieren, die Worked Out Examples durchzugehen und die Aufgaben zu bearbeiten.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Willkommen üëãüèº</span>"
    ]
  },
  {
    "objectID": "regression.html",
    "href": "regression.html",
    "title": "2¬† Einfache und Multiple lineare Regression",
    "section": "",
    "text": "2.1 Einfache lineare Regression\nIn diesem Unterkapitel soll in die einfache lineare Regression eingef√ºhrt werden. Dazu dient ein Erkl√§rvideo gefolgt von Aufgaben.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Einfache und Multiple lineare Regression</span>"
    ]
  },
  {
    "objectID": "regression.html#einfache-lineare-regression",
    "href": "regression.html#einfache-lineare-regression",
    "title": "2¬† Einfache und Multiple lineare Regression",
    "section": "",
    "text": "2.1.1 Erkl√§rvideo\n\n\n\n\n\n\n\n\n\n\n\nDatengrundlage\n\n\n\n\n\nWir werden in diesen Workshop mit den Scientific Usefiles des STAR-Projektes (Achilles u.¬†a. 1985) arbeiten. Im STAR-Projekt standen die folgenden Forschungsfragen im Vordergrund:\n\nWhat are the effects of a reduced class size on the achievement (normed and criterion tests) and development (self-concept, attendance, etc.) of students in public elementary school grades (K-3)?\nIs there a cumulative effects of being in a small class over an extended time (4 years) as compared with a one-year effect for students in a small class for one year?\nDoes a training program designed to help teachers take maximum advantage of small classes, or to use aides effectively, improve student performance as compared with teachers who have no special preparation for their altered conditions?\n\nDie entsprechenden Variablenbezeichnungen sowie die Kodierung der Variablenauspr√§gungen werden in den jeweiligen Beispielen beschrieben.\n\n\n\n\n\n2.1.2 Worked Out Example\nIm ersten Worked Out Example wollen wir der Frage nachgehen, inwiefern die tats√§chliche Klassengr√∂√üe mit der Leistung in einem standardisierten Mathematiktest assoziiert ist.\n\n2.1.2.1 Plot\nIn einem ersten Schritt (der ganz generell immer zu empfehlen ist) plotten wir die Rohdaten. Um keine Probleme mit geclusterten Daten zu bekommen, verwenden wir aus jeder Klasse nur eine:n zuf√§llig gezogenen Sch√ºler:in. Ein .sav-file, das die notwendigen Variablen enth√§lt, kann hier heruntergeladen werden.\n\nlibrary(sjPlot)\nlibrary(bayestestR)\n\n# read the aggregated data\ndata_star_g3aggregated &lt;- read_spss(\"data/data_star_sampled.sav\")\n\n# plot rawdata\nggplot(data_star_g3aggregated,                       # the used data set\n       aes(g3classsize, g3tmathss)) +                # define x- and y-axis\n    geom_jitter() +                                  # add jittered points\n    geom_rug(position = position_jitter(), \n             alpha = .2) +                           # add rug at margins\n    stat_smooth(method = \"linear\") +                 # add linear smoother\n    theme_minimal()                                  # make appearance \"clearer\"\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 37 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Computation failed in `stat_smooth()`.\nCaused by error in `get()`:\n! object 'linear' of mode 'function' was not found\n\n\nWarning: Removed 37 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n2.1.2.2 Nicht-Standardisierte Regression\nUm nun eine einfache lineare Regression zu sch√§tzen, verwendet man in R die lm() Syntax. Links der Tilde ~ steht die abh√§ngige Variable, rechts davon die unabh√§ngige.\n\nmod00 &lt;- lm(g3tmathss ~ g3classsize, \n            data = data_star_g3aggregated)\n\nEine √úbersicht √ºber das Modell bekommt man, wenn man das Objekt mod00 der der Funktion summary() √ºbergibt.\n\nsummary(mod00)\n\n\nCall:\nlm(formula = g3tmathss ~ g3classsize, data = data_star_g3aggregated)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-109.454  -29.725   -0.446   28.743  102.955 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 642.8773    10.1681  63.225   &lt;2e-16 ***\ng3classsize  -0.9015     0.4939  -1.825   0.0689 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 38.54 on 298 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.01106,   Adjusted R-squared:  0.007739 \nF-statistic: 3.332 on 1 and 298 DF,  p-value: 0.06895\n\n\nDie (lineare) Funktionsgleichung kann man sich mit der Funktion extract_eq() aus dem Paket equatiomatic ausgeben lassen. Mit der Option use_coefs = T setzt man die gesch√§tzten Werte f√ºr die Parameter ein.\n\nlibrary(equatiomatic)\nextract_eq(mod00)\n\n\\[\n\\operatorname{g3tmathss} = \\alpha + \\beta_{1}(\\operatorname{g3classsize}) + \\epsilon\n\\]\n\nextract_eq(mod00, use_coefs = T)\n\n\\[\n\\operatorname{\\widehat{g3tmathss}} = 642.88 - 0.9(\\operatorname{g3classsize})\n\\]\n\n\nDer Steigungskoeffizient betr√§gt \\(\\approx .90\\). Unterscheiden sich zwei Klassen um eine:n Sch√ºler:in sch√§tzt das Modell die Differenz im Mathematikscore auf \\(-.90\\). Das Intercept wird auf \\(638.9\\) gesch√§tzt. Eine (hypothetische) Klasse mit 0 Sch√ºler:innen h√§tte also einen durchschnittlichen Mathematikleistungsscore von \\(638.9\\).\nDie drei Sternchen am rechten Rand des summary() Outputs zeigen an, dass die p-Werte f√ºr die Punktnullhypothesen \\[\nH_0\\text{: Intercept} = 0\n\\] \\[\nH_0\\text{: Slope} = 0\n\\] signifikant sind. Es macht also Sinn diese zu verwerfen.\n\n\n2.1.2.3 Standardisierte Regression\nEine standardisierte lineare Regression setzt wie im Video erkl√§rt voraus, dass alle Variablen z-standardisiert sind. Liegen unvollst√§ndige Daten vor, ist es wichtig diese Standardisierung erst nach dem fallweisen Ausschluss dieser fehlenden Daten vorzunehmen.\n\nmod01 &lt;- lm(scale(g3tmathss) ~ scale(g3classsize), \n            data = data_star_g3aggregated |&gt; \n                # filter rows if g3tmathss or g3classsize is NA\n                filter(!(is.na(g3tmathss) | is.na(g3classsize))))\nsummary(mod01)\n\n\nCall:\nlm(formula = scale(g3tmathss) ~ scale(g3classsize), data = filter(data_star_g3aggregated, \n    !(is.na(g3tmathss) | is.na(g3classsize))))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.82930 -0.76836 -0.01154  0.74300  2.66132 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)         4.284e-16  5.751e-02   0.000   1.0000  \nscale(g3classsize) -1.052e-01  5.761e-02  -1.825   0.0689 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9961 on 298 degrees of freedom\nMultiple R-squared:  0.01106,   Adjusted R-squared:  0.007739 \nF-statistic: 3.332 on 1 and 298 DF,  p-value: 0.06895\n\nextract_eq(mod01, use_coefs = T)\n\n\\[\n\\operatorname{\\widehat{scale(g3tmathss)}} = 0 - 0.11(\\operatorname{scale(g3classsize)})\n\\]\n\n\nDas Intercept wird auf einen Wert mit 14 Nullen nach dem Komma gesch√§tzt, ist also erwartungskonform quasi gleich null und nicht-signifikant. Der Steigungskoeffizient betr√§gt \\(\\approx .20\\) und liegt nach den Cohen Benchmarks (Cohen 1988) im Bereich kleiner bis moderater Effekte.\nEs gibt Pakete, die darauf spezialisiert sind (mehrere) Regressionsmodelle zusammengefasst √ºbersichtlich in Tabellen darzustellen. Mit der folgenden Syntax bekommt man etwa nicht nur standardisierte und unstandardisierte Koeffizienten, sondern auch deren Konfidenzintervalle, beides auf eine sinnvolle Nachkommestellenanzahl gerundet:\n\nlibrary(sjPlot)\ntab_model(mod00, show.ci = .95, show.std = T)\n\n\n\n\n\n\n\n\n\n\n\n\n\n¬†\nTOTAL MATH SCALE SCORE\nSAT GRADE 3\n\n\nPredictors\nEstimates\nstd. Beta\nCI\nstandardized CI\np\n\n\n(Intercept)\n642.88\n0.00\n622.87¬†‚Äì¬†662.89\n-0.11¬†‚Äì¬†0.11\n&lt;0.001\n\n\nCLASS SIZE GRADE 3\n-0.90\n-0.11\n-1.87¬†‚Äì¬†0.07\n-0.22¬†‚Äì¬†0.01\n0.069\n\n\nObservations\n300\n\n\nR2 / R2 adjusted\n0.011 / 0.008",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Einfache und Multiple lineare Regression</span>"
    ]
  },
  {
    "objectID": "regression.html#aufgabe",
    "href": "regression.html#aufgabe",
    "title": "2¬† Einfache und Multiple lineare Regression",
    "section": "2.2 Aufgabe",
    "text": "2.2 Aufgabe\n\nAufgabeL√∂sungshinweiseL√∂sung\n\n\nDas STAR-Experiment variierte die Klassengr√∂√üen experimentell in K, G1, G2, G3 & G4. Daten wurden aber bis zu High School erhoben.\nDie Variable g4tmathss etwa erfasst die Mathematikleistung in Klasse 4, die Variablen g4pteffr und g4ptvalu den von Lehrkr√§ften eingesch√§tzte schulische Leistungsbereitschaft bzw. die Wertsch√§tzung schulischer Inhalte.\nWie gro√ü sch√§tzt ihr die Effekte der Pr√§diktoren g4pteffr und g4ptvalu ein? Berechnet die standardisierten und nicht-standardisierten Regressionsmodelle und diskutiert die interne, externe und Konstruktvalidit√§t der so erhaltenen Befunde.\n\n\n\nlibrary(haven)\nlibrary(sjPlot)\ndata_star_sampled &lt;- read_spss(\"data/data_star_sampled.sav\")\n\nmod02 &lt;- lm(g4tmathss ~ g4pteffr, data = data_star_sampled)\nmod03 &lt;- lm(g4tmathss ~ g4ptvalu, data = data_star_sampled)\n\ntab_model(mod02, mod03, show.std = T, show.ci = F)\n\n\n\n\ntab_model(mod02, mod03, show.std = T, show.ci = F)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¬†\nTOTAL MATH SCALE SCORE\nCTBS GRADE 4\nTOTAL MATH SCALE SCORE\nCTBS GRADE 4\n\n\nPredictors\nEstimates\nstd. Beta\np\nEstimates\nstd. Beta\np\n\n\n(Intercept)\n595.70\n0.00\n&lt;0.001\n623.37\n0.00\n&lt;0.001\n\n\nGRADE 4 PARTICIPATION\nSUBSCORE: EFFORT\n2.44\n0.60\n&lt;0.001\n\n\n\n\n\nGRADE 4 PARTICIPATION\nSUBSCORE: VALUE\n\n\n\n7.07\n0.37\n&lt;0.001\n\n\nObservations\n106\n106\n\n\nR2 / R2 adjusted\n0.357 / 0.351\n0.137 / 0.129\n\n\n\n\n\n\n\n\nDer standardisierte Regressionskoeffizient der Effort-Skala ist mit .58 enorm gro√ü ausgepr√§gt und auch der Effekt des Pr√§diktors value ist substantiell. Beachtet werden muss allerdings, dass die p-Werte nichts √ºber die Sicherheit der Unterschiedlichkeit der beiden Steigungsparameter aussagt. Getestet wurde jeweils wieder nur die Nullhypothese eines Nulleffekts. Zu kritisieren sind hier sicher interne und Konstruktvalidit√§t. Effort und Value der Sch√ºler:innen wurden von den Lehrkr√§ften ohne vorherige Raterschulung eingesch√§tzt. Daher ist anzunehmen, dass dieses Rating auch durch die Leistung der Sch√ºler:innen verzerrt ist (z.B. im Sinne eines Haloeffekts, Dennis 2007). Die interne Validit√§t der Schlussfolgerung aus diesen Regressionsmodellen ist schwach, da es sich nur um querschnittliche Daten handelt und die Auspr√§gung der unabh√§ngigen Variable nicht randomisiert wurde.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Einfache und Multiple lineare Regression</span>"
    ]
  },
  {
    "objectID": "regression.html#multiple-lineare-regression",
    "href": "regression.html#multiple-lineare-regression",
    "title": "2¬† Einfache und Multiple lineare Regression",
    "section": "2.3 Multiple lineare Regression",
    "text": "2.3 Multiple lineare Regression\nIn diesem Unterkapitel soll in die multiple Regression eingef√ºhrt werden. Dazu dient ebenfalls ein Erkl√§rvideo gefolgt von Aufgaben.\n\n2.3.1 Erkl√§rvideo\n\n\n\n\n\n\n\n2.3.2 Erkl√§rvideo\nDie standardisierten Steigungskoeffizeinten \\(\\beta_i\\) stellen ja eine Effektst√§rke der partiellen Assoziation des Pr√§diktors \\(i\\) mit der abh√§ngigen Variable dar. Nimmt man mehrere Pr√§diktoren auf, kann der Determinationskoeffizient \\(R^2\\) eine Effektst√§rke f√ºr die G√ºte des Gesamtmodells darstellen.\n\n\n\n\n\n\n\n2.3.3 Worked Out Example\nIn der √úbungsaufgabe zur einfachen linearen Regression haben wir vermutet, dass Einsch√§tzung von Effort und Value durch die Lehrkraft von der Leistung der Lernenden gef√§rbt sein k√∂nnte. W√§re dem so, sollte man ein Sinken der Pr√§diktionskraft des Pr√§diktors Value nach Adjustierung um die Vorjahresleistung beobachten.\n\ntab_model(\n    lm(g4tmathss ~ g4ptvalu, data = data_star_sampled),\n    lm(g4tmathss ~ g4ptvalu + g3tmathss, data = data_star_sampled),\n    show.ci = F, show.std = T\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¬†\nTOTAL MATH SCALE SCORE\nCTBS GRADE 4\nTOTAL MATH SCALE SCORE\nCTBS GRADE 4\n\n\nPredictors\nEstimates\nstd. Beta\np\nEstimates\nstd. Beta\np\n\n\n(Intercept)\n623.37\n0.00\n&lt;0.001\n183.81\n-0.00\n&lt;0.001\n\n\nGRADE 4 PARTICIPATION\nSUBSCORE: VALUE\n7.07\n0.37\n&lt;0.001\n1.37\n0.07\n0.332\n\n\nTOTAL MATH SCALE SCORE\nSAT GRADE 3\n\n\n\n0.81\n0.70\n&lt;0.001\n\n\nObservations\n106\n103\n\n\nR2 / R2 adjusted\n0.137 / 0.129\n0.536 / 0.527\n\n\n\n\n\n\n\n\nDies ist tats√§chlich der Fall. Der standardisierte Regressionskoeffizient sinkt von .45 auf .18.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Einfache und Multiple lineare Regression</span>"
    ]
  },
  {
    "objectID": "regression.html#aufgabe-2",
    "href": "regression.html#aufgabe-2",
    "title": "2¬† Einfache und Multiple lineare Regression",
    "section": "2.4 Aufgabe",
    "text": "2.4 Aufgabe\n\nAufgabeL√∂sungshinweiseL√∂sung\n\n\nUntersucht, inwiefern die ebenfalls Lehrer:inneingesch√§tzte Variable Initiative (z.B. ‚Äúparticipates actively in class discussions‚Äù) g4ptinit die Mathematikleistung in Klasse 4 g4tmathss pr√§diziert und inwiefern sich der Effekt nach Adjustierung der Vortestleistung g3tmathss √§ndert\n\n\n\nlm(g4tmathss ~ g4ptinit, data = data_star_sampled)\nlm(g4tmathss ~ g4ptinit + g3tmathss, data = data_star_sampled)\n\n\n\n\ntab_model(\n    lm(g4tmathss ~ g4ptinit, data = data_star_sampled),\n    lm(g4tmathss ~ g4ptinit + g3tmathss, data = data_star_sampled),\n    show.std = T, show.ci = F)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¬†\nTOTAL MATH SCALE SCORE\nCTBS GRADE 4\nTOTAL MATH SCALE SCORE\nCTBS GRADE 4\n\n\nPredictors\nEstimates\nstd. Beta\np\nEstimates\nstd. Beta\np\n\n\n(Intercept)\n607.56\n0.00\n&lt;0.001\n255.34\n-0.00\n&lt;0.001\n\n\nGRADE 4 PARTICIPATION\nSUBSCORE: INITIATIVE\n3.89\n0.64\n&lt;0.001\n1.73\n0.28\n0.001\n\n\nTOTAL MATH SCALE SCORE\nSAT GRADE 3\n\n\n\n0.65\n0.56\n&lt;0.001\n\n\nObservations\n106\n103\n\n\nR2 / R2 adjusted\n0.405 / 0.399\n0.583 / 0.575\n\n\n\n\n\n\n\n\nDer standardisierte Regressionskoeffizient der Initiative-Skala ist mit .49 gro√ü ausgepr√§gt. Nach Adjustierung um die Vorjahrestestleistung, sinkt die pr√§diktive Kraft deutlich, es ist aber weiterhin ein substantieller Effekt zu beobachten.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Einfache und Multiple lineare Regression</span>"
    ]
  },
  {
    "objectID": "regression.html#weiterf√ºhrende-literatur",
    "href": "regression.html#weiterf√ºhrende-literatur",
    "title": "2¬† Einfache und Multiple lineare Regression",
    "section": "2.5 Weiterf√ºhrende Literatur",
    "text": "2.5 Weiterf√ºhrende Literatur\n\n\n\n\n\n\nLiteraturempfehlungen zum Thema Regression\n\n\n\n\n\n\nEid, M., Gollwitzer, M., & Schmitt, M. (2013). Statistik und Forschungsmethoden: Lehrbuch. Mit Online-Materialien (3. Aufl.). Beltz.\nGelman, A., Hill, J., & Vehtari, A. (2020). Regression and Other Stories (1. Aufl.). Cambridge University Press. https://doi.org/10.1017/9781139161879\n\n\n\n\n\n\n\n\nAchilles, C. M., Helen Pate Bain, F. Bellot, J. Boyd-Zaharias, J. Finn, J. Folger, John Johnston, und Elizabeth Word. 1985. ‚ÄûThe State of Tennessee‚Äôs Student/Teacher Achievement Ratio (STAR) Project‚Äú. Technical {{Report}}. Tennessee State Department of Educatbn.\n\n\nCohen, Jacob. 1988. Statistical Power Analysis for the Behavioral Sciences. 2. Aufl. New Jersey: Lawrence Erlbaum.\n\n\nDennis, Ian. 2007. ‚ÄûHalo Effects in Grading Student Projects‚Äú. Journal of Applied Psychology 92 (4): 1169‚Äì76. https://doi.org/10.1037/0021-9010.92.4.1169.\n\n\nMoosbrugger, Helfried. 2011. Lineare Modelle: Regressions- und Varianzanalysen ; mit einem Anhang √ºber Matrixalgebra. 4., vollst√§ndig √ºberarbeitete und erg√§nzte Auflage. Psychologie Lehrtexte. Bern: Verlag Hans Huber.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Einfache und Multiple lineare Regression</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Literatur",
    "section": "",
    "text": "Achilles, C. M., Helen Pate Bain, F. Bellot, J. Boyd-Zaharias, J. Finn,\nJ. Folger, John Johnston, and Elizabeth Word. 1985. ‚ÄúThe State of\nTennessee‚Äôs Student/Teacher Achievement Ratio\n(STAR) Project.‚Äù Technical {{Report}}.\nTennessee State Department of Educatbn.\n\n\nCohen, Jacob. 1988. Statistical Power Analysis for the Behavioral\nSciences. 2nd ed. New Jersey: Lawrence\nErlbaum.\n\n\nDennis, Ian. 2007. ‚ÄúHalo Effects in Grading Student\nProjects.‚Äù Journal of Applied Psychology 92 (4):\n1169‚Äì76. https://doi.org/10.1037/0021-9010.92.4.1169.\n\n\nMoosbrugger, Helfried. 2011. Lineare Modelle: Regressions- und\nVarianzanalysen ; mit einem Anhang √ºber\nMatrixalgebra. 4., vollst√§ndig\n√ºberarbeitete und erg√§nzte Auflage.\nPsychologie Lehrtexte. Bern: Verlag Hans\nHuber.",
    "crumbs": [
      "Literatur"
    ]
  }
]